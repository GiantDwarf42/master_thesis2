{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Sim study analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Specify the directory containing the CSV files\n",
    "directory = r'G:\\My Drive\\Studium\\UNIGE_Master\\Thesis\\Master_Thesis\\Data\\Gaussian_Data_raw'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: List all files in the directory\n",
    "all_files = os.listdir(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Group files by their common components except the ID\n",
    "file_groups = defaultdict(list)\n",
    "for file in all_files:\n",
    "    if file.endswith('.csv'):\n",
    "        parts = file.split('_')\n",
    "        key = '_'.join(parts[:-1])  # Everything except the last part (ID)\n",
    "        file_groups[key].append(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded and processed 1 simulation\n",
      "loaded and processed 2 simulation\n",
      "loaded and processed 3 simulation\n",
      "loaded and processed 4 simulation\n",
      "loaded and processed 5 simulation\n",
      "loaded and processed 6 simulation\n",
      "loaded and processed 7 simulation\n",
      "loaded and processed 8 simulation\n",
      "loaded and processed 9 simulation\n",
      "loaded and processed 10 simulation\n",
      "loaded and processed 11 simulation\n",
      "loaded and processed 12 simulation\n",
      "loaded and processed 13 simulation\n",
      "loaded and processed 14 simulation\n",
      "loaded and processed 15 simulation\n",
      "loaded and processed 16 simulation\n",
      "loaded and processed 17 simulation\n",
      "loaded and processed 18 simulation\n",
      "loaded and processed 19 simulation\n",
      "loaded and processed 20 simulation\n",
      "loaded and processed 21 simulation\n",
      "loaded and processed 22 simulation\n",
      "loaded and processed 23 simulation\n",
      "loaded and processed 24 simulation\n",
      "loaded and processed 25 simulation\n",
      "loaded and processed 26 simulation\n",
      "loaded and processed 27 simulation\n",
      "loaded and processed 28 simulation\n",
      "loaded and processed 29 simulation\n",
      "loaded and processed 30 simulation\n",
      "loaded and processed 31 simulation\n",
      "loaded and processed 32 simulation\n",
      "loaded and processed 33 simulation\n",
      "loaded and processed 34 simulation\n",
      "loaded and processed 35 simulation\n",
      "loaded and processed 36 simulation\n",
      "loaded and processed 37 simulation\n",
      "loaded and processed 38 simulation\n",
      "loaded and processed 39 simulation\n",
      "loaded and processed 40 simulation\n",
      "loaded and processed 41 simulation\n",
      "loaded and processed 42 simulation\n",
      "loaded and processed 43 simulation\n",
      "loaded and processed 44 simulation\n",
      "loaded and processed 45 simulation\n",
      "loaded and processed 46 simulation\n",
      "loaded and processed 47 simulation\n",
      "loaded and processed 48 simulation\n",
      "loaded and processed 49 simulation\n",
      "loaded and processed 50 simulation\n",
      "loaded and processed 51 simulation\n",
      "loaded and processed 52 simulation\n",
      "loaded and processed 53 simulation\n",
      "loaded and processed 54 simulation\n",
      "loaded and processed 55 simulation\n",
      "loaded and processed 56 simulation\n",
      "loaded and processed 57 simulation\n",
      "loaded and processed 58 simulation\n",
      "loaded and processed 59 simulation\n",
      "loaded and processed 60 simulation\n",
      "loaded and processed 61 simulation\n",
      "loaded and processed 62 simulation\n",
      "loaded and processed 63 simulation\n",
      "loaded and processed 64 simulation\n",
      "loaded and processed 65 simulation\n",
      "loaded and processed 66 simulation\n",
      "loaded and processed 67 simulation\n",
      "loaded and processed 68 simulation\n",
      "loaded and processed 69 simulation\n",
      "loaded and processed 70 simulation\n",
      "loaded and processed 71 simulation\n",
      "loaded and processed 72 simulation\n",
      "loaded and processed 73 simulation\n",
      "loaded and processed 74 simulation\n",
      "loaded and processed 75 simulation\n",
      "loaded and processed 76 simulation\n",
      "loaded and processed 77 simulation\n",
      "loaded and processed 78 simulation\n",
      "loaded and processed 79 simulation\n",
      "loaded and processed 80 simulation\n",
      "loaded and processed 81 simulation\n",
      "loaded and processed 82 simulation\n",
      "loaded and processed 83 simulation\n",
      "loaded and processed 84 simulation\n",
      "loaded and processed 85 simulation\n",
      "loaded and processed 86 simulation\n",
      "loaded and processed 87 simulation\n",
      "loaded and processed 88 simulation\n",
      "loaded and processed 89 simulation\n",
      "loaded and processed 90 simulation\n",
      "loaded and processed 91 simulation\n",
      "loaded and processed 92 simulation\n",
      "loaded and processed 93 simulation\n",
      "loaded and processed 94 simulation\n",
      "loaded and processed 95 simulation\n",
      "loaded and processed 96 simulation\n",
      "loaded and processed 97 simulation\n",
      "loaded and processed 98 simulation\n",
      "loaded and processed 99 simulation\n",
      "loaded and processed 100 simulation\n",
      "loaded and processed 101 simulation\n",
      "loaded and processed 102 simulation\n",
      "loaded and processed 103 simulation\n",
      "loaded and processed 104 simulation\n",
      "loaded and processed 105 simulation\n",
      "loaded and processed 106 simulation\n",
      "loaded and processed 107 simulation\n",
      "loaded and processed 108 simulation\n",
      "loaded and processed 109 simulation\n",
      "loaded and processed 110 simulation\n",
      "loaded and processed 111 simulation\n",
      "loaded and processed 112 simulation\n",
      "loaded and processed 113 simulation\n",
      "loaded and processed 114 simulation\n",
      "loaded and processed 115 simulation\n",
      "loaded and processed 116 simulation\n",
      "loaded and processed 117 simulation\n",
      "loaded and processed 118 simulation\n",
      "loaded and processed 119 simulation\n",
      "loaded and processed 120 simulation\n",
      "loaded and processed 121 simulation\n",
      "loaded and processed 122 simulation\n",
      "loaded and processed 123 simulation\n",
      "loaded and processed 124 simulation\n",
      "loaded and processed 125 simulation\n",
      "loaded and processed 126 simulation\n",
      "loaded and processed 127 simulation\n",
      "loaded and processed 128 simulation\n",
      "loaded and processed 129 simulation\n",
      "loaded and processed 130 simulation\n",
      "loaded and processed 131 simulation\n",
      "loaded and processed 132 simulation\n",
      "loaded and processed 133 simulation\n",
      "loaded and processed 134 simulation\n",
      "loaded and processed 135 simulation\n",
      "loaded and processed 136 simulation\n",
      "loaded and processed 137 simulation\n",
      "loaded and processed 138 simulation\n",
      "loaded and processed 139 simulation\n",
      "loaded and processed 140 simulation\n",
      "loaded and processed 141 simulation\n",
      "loaded and processed 142 simulation\n",
      "loaded and processed 143 simulation\n",
      "loaded and processed 144 simulation\n",
      "loaded and processed 145 simulation\n",
      "loaded and processed 146 simulation\n",
      "loaded and processed 147 simulation\n",
      "loaded and processed 148 simulation\n",
      "loaded and processed 149 simulation\n",
      "loaded and processed 150 simulation\n",
      "loaded and processed 151 simulation\n",
      "loaded and processed 152 simulation\n",
      "loaded and processed 153 simulation\n",
      "loaded and processed 154 simulation\n",
      "loaded and processed 155 simulation\n",
      "loaded and processed 156 simulation\n",
      "loaded and processed 157 simulation\n",
      "loaded and processed 158 simulation\n",
      "loaded and processed 159 simulation\n",
      "loaded and processed 160 simulation\n",
      "loaded and processed 161 simulation\n",
      "loaded and processed 162 simulation\n",
      "loaded and processed 163 simulation\n",
      "loaded and processed 164 simulation\n",
      "loaded and processed 165 simulation\n",
      "loaded and processed 166 simulation\n",
      "loaded and processed 167 simulation\n",
      "loaded and processed 168 simulation\n",
      "loaded and processed 169 simulation\n",
      "loaded and processed 170 simulation\n",
      "loaded and processed 171 simulation\n",
      "loaded and processed 172 simulation\n",
      "loaded and processed 173 simulation\n",
      "loaded and processed 174 simulation\n",
      "loaded and processed 175 simulation\n",
      "loaded and processed 176 simulation\n",
      "loaded and processed 177 simulation\n",
      "loaded and processed 178 simulation\n",
      "loaded and processed 179 simulation\n",
      "loaded and processed 180 simulation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Read and stack files within each group, adding columns for the components\n",
    "stacked_dataframes = []\n",
    "count = 0\n",
    "for key, files in file_groups.items():\n",
    "    dataframes = []\n",
    "    for file in sorted(files, key=lambda x: int(x.split('_')[-1].split('.')[0][2:])):\n",
    "        df = pd.read_csv(os.path.join(directory, file),index_col=False)\n",
    "    \n",
    "        df = df.drop([\"Unnamed: 0\", \"b\"], axis=1)\n",
    "        \n",
    "        # Extract components from the filename\n",
    "        parts = file.split('_')\n",
    "        components = {\n",
    "            'mu': np.repeat(int(parts[0][2:]), df.shape[0]),\n",
    "            'sigma':np.repeat( int(parts[1][5:]),df.shape[0]),\n",
    "            'xsize': np.repeat(int(parts[2][5:]),df.shape[0]),\n",
    "            'ysize': np.repeat(int(parts[3][5:]),df.shape[0]),\n",
    "            'b': np.repeat(int(parts[4][5:]),df,shape[0]),\n",
    "            'ID': np.repeat(int(parts[5].split('.')[0][2:]),df.shape[0])  # Remove the .csv part\n",
    "        }\n",
    "\n",
    "        flag_df = pd.DataFrame.from_dict(components)\n",
    "\n",
    "        df_flags_attached = pd.concat([df,flag_df], axis=1)\n",
    "        \n",
    "        dataframes.append(df_flags_attached)\n",
    "\n",
    "    \n",
    "    # Concatenate dataframes\n",
    "    stacked_df = pd.concat(dataframes, ignore_index=True)\n",
    "    stacked_dataframes.append(stacked_df)\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "    print(f\"loaded and processed {count} simulation\")\n",
    "\n",
    "\n",
    "# Now `stacked_dataframes` is a list of stacked dataframes with additional columns for the components.\n",
    "\n",
    "# stack everything => same name for ram issue...\n",
    "stacked_dataframes = pd.concat(stacked_dataframes)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 40500000 entries, 0 to 224999\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   mu_hat         float64\n",
      " 1   sigma_hat      float64\n",
      " 2   MMD            float64\n",
      " 3   b              float64\n",
      " 4   time           float64\n",
      " 5   mu_hat_MLE     float64\n",
      " 6   sigma_hat_MLE  float64\n",
      " 7   mu             int32  \n",
      " 8   sigma          int32  \n",
      " 9   xsize          int32  \n",
      " 10  ysize          int32  \n",
      " 11  ID             int32  \n",
      "dtypes: float64(7), int32(5)\n",
      "memory usage: 3.2 GB\n"
     ]
    }
   ],
   "source": [
    "stacked_dataframes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:5: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:5: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\nickk\\AppData\\Local\\Temp\\ipykernel_22732\\1768519353.py:5: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  stacked_dataframes.to_csv(f\"{saving_path}\\{file_name}.csv\")\n"
     ]
    }
   ],
   "source": [
    "# 5 step: safe the processed DataFrame\n",
    "file_name = \"Gauss_Processed_data\"\n",
    "saving_path = r\"G:\\My Drive\\Studium\\UNIGE_Master\\Thesis\\Master_Thesis\\Data\\Gaussian_Data_processed\"\n",
    "\n",
    "stacked_dataframes.to_csv(f\"{saving_path}\\{file_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:1: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\nickk\\AppData\\Local\\Temp\\ipykernel_22732\\2723069148.py:1: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  stacked_dataframes.to_pickle(f\"{saving_path}\\{file_name}.pkl\")\n"
     ]
    }
   ],
   "source": [
    "stacked_dataframes.to_pickle(f\"{saving_path}\\{file_name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
