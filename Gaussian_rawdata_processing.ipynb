{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Sim study analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Specify the directory containing the CSV files\n",
    "directory = r'G:\\My Drive\\Studium\\UNIGE_Master\\Thesis\\Master_Thesis\\Data\\Gaussian_Data_raw'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: List all files in the directory\n",
    "all_files = os.listdir(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Group files by their common components except the ID\n",
    "file_groups = defaultdict(list)\n",
    "for file in all_files:\n",
    "    if file.endswith('.csv'):\n",
    "        parts = file.split('_')\n",
    "        key = '_'.join(parts[:-1])  # Everything except the last part (ID)\n",
    "        file_groups[key].append(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded and processed 1 simulations\n",
      "loaded and processed 2 simulations\n",
      "loaded and processed 3 simulations\n",
      "loaded and processed 4 simulations\n",
      "loaded and processed 5 simulations\n",
      "loaded and processed 6 simulations\n",
      "loaded and processed 7 simulations\n",
      "loaded and processed 8 simulations\n",
      "loaded and processed 9 simulations\n",
      "loaded and processed 10 simulations\n",
      "loaded and processed 11 simulations\n",
      "loaded and processed 12 simulations\n",
      "loaded and processed 13 simulations\n",
      "loaded and processed 14 simulations\n",
      "loaded and processed 15 simulations\n",
      "loaded and processed 16 simulations\n",
      "loaded and processed 17 simulations\n",
      "loaded and processed 18 simulations\n",
      "loaded and processed 19 simulations\n",
      "loaded and processed 20 simulations\n",
      "loaded and processed 21 simulations\n",
      "loaded and processed 22 simulations\n",
      "loaded and processed 23 simulations\n",
      "loaded and processed 24 simulations\n",
      "loaded and processed 25 simulations\n",
      "loaded and processed 26 simulations\n",
      "loaded and processed 27 simulations\n",
      "loaded and processed 28 simulations\n",
      "loaded and processed 29 simulations\n",
      "loaded and processed 30 simulations\n",
      "loaded and processed 31 simulations\n",
      "loaded and processed 32 simulations\n",
      "loaded and processed 33 simulations\n",
      "loaded and processed 34 simulations\n",
      "loaded and processed 35 simulations\n",
      "loaded and processed 36 simulations\n",
      "loaded and processed 37 simulations\n",
      "loaded and processed 38 simulations\n",
      "loaded and processed 39 simulations\n",
      "loaded and processed 40 simulations\n",
      "loaded and processed 41 simulations\n",
      "loaded and processed 42 simulations\n",
      "loaded and processed 43 simulations\n",
      "loaded and processed 44 simulations\n",
      "loaded and processed 45 simulations\n",
      "loaded and processed 46 simulations\n",
      "loaded and processed 47 simulations\n",
      "loaded and processed 48 simulations\n",
      "loaded and processed 49 simulations\n",
      "loaded and processed 50 simulations\n",
      "loaded and processed 51 simulations\n",
      "loaded and processed 52 simulations\n",
      "loaded and processed 53 simulations\n",
      "loaded and processed 54 simulations\n",
      "loaded and processed 55 simulations\n",
      "loaded and processed 56 simulations\n",
      "loaded and processed 57 simulations\n",
      "loaded and processed 58 simulations\n",
      "loaded and processed 59 simulations\n",
      "loaded and processed 60 simulations\n",
      "loaded and processed 61 simulations\n",
      "loaded and processed 62 simulations\n",
      "loaded and processed 63 simulations\n",
      "loaded and processed 64 simulations\n",
      "loaded and processed 65 simulations\n",
      "loaded and processed 66 simulations\n",
      "loaded and processed 67 simulations\n",
      "loaded and processed 68 simulations\n",
      "loaded and processed 69 simulations\n",
      "loaded and processed 70 simulations\n",
      "loaded and processed 71 simulations\n",
      "loaded and processed 72 simulations\n",
      "loaded and processed 73 simulations\n",
      "loaded and processed 74 simulations\n",
      "loaded and processed 75 simulations\n",
      "loaded and processed 76 simulations\n",
      "loaded and processed 77 simulations\n",
      "loaded and processed 78 simulations\n",
      "loaded and processed 79 simulations\n",
      "loaded and processed 80 simulations\n",
      "loaded and processed 81 simulations\n",
      "loaded and processed 82 simulations\n",
      "loaded and processed 83 simulations\n",
      "loaded and processed 84 simulations\n",
      "loaded and processed 85 simulations\n",
      "loaded and processed 86 simulations\n",
      "loaded and processed 87 simulations\n",
      "loaded and processed 88 simulations\n",
      "loaded and processed 89 simulations\n",
      "loaded and processed 90 simulations\n",
      "loaded and processed 91 simulations\n",
      "loaded and processed 92 simulations\n",
      "loaded and processed 93 simulations\n",
      "loaded and processed 94 simulations\n",
      "loaded and processed 95 simulations\n",
      "loaded and processed 96 simulations\n",
      "loaded and processed 97 simulations\n",
      "loaded and processed 98 simulations\n",
      "loaded and processed 99 simulations\n",
      "loaded and processed 100 simulations\n",
      "loaded and processed 101 simulations\n",
      "loaded and processed 102 simulations\n",
      "loaded and processed 103 simulations\n",
      "loaded and processed 104 simulations\n",
      "loaded and processed 105 simulations\n",
      "loaded and processed 106 simulations\n",
      "loaded and processed 107 simulations\n",
      "loaded and processed 108 simulations\n",
      "loaded and processed 109 simulations\n",
      "loaded and processed 110 simulations\n",
      "loaded and processed 111 simulations\n",
      "loaded and processed 112 simulations\n",
      "loaded and processed 113 simulations\n",
      "loaded and processed 114 simulations\n",
      "loaded and processed 115 simulations\n",
      "loaded and processed 116 simulations\n",
      "loaded and processed 117 simulations\n",
      "loaded and processed 118 simulations\n",
      "loaded and processed 119 simulations\n",
      "loaded and processed 120 simulations\n",
      "loaded and processed 121 simulations\n",
      "loaded and processed 122 simulations\n",
      "loaded and processed 123 simulations\n",
      "loaded and processed 124 simulations\n",
      "loaded and processed 125 simulations\n",
      "loaded and processed 126 simulations\n",
      "loaded and processed 127 simulations\n",
      "loaded and processed 128 simulations\n",
      "loaded and processed 129 simulations\n",
      "loaded and processed 130 simulations\n",
      "loaded and processed 131 simulations\n",
      "loaded and processed 132 simulations\n",
      "loaded and processed 133 simulations\n",
      "loaded and processed 134 simulations\n",
      "loaded and processed 135 simulations\n",
      "loaded and processed 136 simulations\n",
      "loaded and processed 137 simulations\n",
      "loaded and processed 138 simulations\n",
      "loaded and processed 139 simulations\n",
      "loaded and processed 140 simulations\n",
      "loaded and processed 141 simulations\n",
      "loaded and processed 142 simulations\n",
      "loaded and processed 143 simulations\n",
      "loaded and processed 144 simulations\n",
      "loaded and processed 145 simulations\n",
      "loaded and processed 146 simulations\n",
      "loaded and processed 147 simulations\n",
      "loaded and processed 148 simulations\n",
      "loaded and processed 149 simulations\n",
      "loaded and processed 150 simulations\n",
      "loaded and processed 151 simulations\n",
      "loaded and processed 152 simulations\n",
      "loaded and processed 153 simulations\n",
      "loaded and processed 154 simulations\n",
      "loaded and processed 155 simulations\n",
      "loaded and processed 156 simulations\n",
      "loaded and processed 157 simulations\n",
      "loaded and processed 158 simulations\n",
      "loaded and processed 159 simulations\n",
      "loaded and processed 160 simulations\n",
      "loaded and processed 161 simulations\n",
      "loaded and processed 162 simulations\n",
      "loaded and processed 163 simulations\n",
      "loaded and processed 164 simulations\n",
      "loaded and processed 165 simulations\n",
      "loaded and processed 166 simulations\n",
      "loaded and processed 167 simulations\n",
      "loaded and processed 168 simulations\n",
      "loaded and processed 169 simulations\n",
      "loaded and processed 170 simulations\n",
      "loaded and processed 171 simulations\n",
      "loaded and processed 172 simulations\n",
      "loaded and processed 173 simulations\n",
      "loaded and processed 174 simulations\n",
      "loaded and processed 175 simulations\n",
      "loaded and processed 176 simulations\n",
      "loaded and processed 177 simulations\n",
      "loaded and processed 178 simulations\n",
      "loaded and processed 179 simulations\n",
      "loaded and processed 180 simulations\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Read and stack files within each group, adding columns for the components\n",
    "stacked_dataframes = []\n",
    "count = 0\n",
    "for key, files in file_groups.items():\n",
    "    dataframes = []\n",
    "    for file in sorted(files, key=lambda x: int(x.split('_')[-1].split('.')[0][2:])):\n",
    "        df = pd.read_csv(os.path.join(directory, file),index_col=False)\n",
    "    \n",
    "        df = df.drop([\"Unnamed: 0\",\"mu_hat_MM\", \"sigma_hat_MM\"], axis=1)\n",
    "        \n",
    "        # Extract components from the filename\n",
    "        parts = file.split('_')\n",
    "        components = {\n",
    "            'mu': np.repeat(int(parts[0][2:]), df.shape[0]),\n",
    "            'sigma':np.repeat( int(parts[1][5:]),df.shape[0]),\n",
    "            'xsize': np.repeat(int(parts[2][5:]),df.shape[0]),\n",
    "            'ysize': np.repeat(int(parts[3][5:]),df.shape[0]),\n",
    "            #'b': parts[4],\n",
    "            'ID': np.repeat(int(parts[5].split('.')[0][2:]),df.shape[0])  # Remove the .csv part\n",
    "        }\n",
    "\n",
    "        flag_df = pd.DataFrame.from_dict(components)\n",
    "\n",
    "        df_flags_attached = pd.concat([df,flag_df], axis=1)\n",
    "        \n",
    "        dataframes.append(df_flags_attached)\n",
    "\n",
    "    \n",
    "    # Concatenate dataframes\n",
    "    stacked_df = pd.concat(dataframes, ignore_index=True)\n",
    "    stacked_dataframes.append(stacked_df)\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "    print(f\"loaded and processed {count} simulations\")\n",
    "\n",
    "    # if count == 5:\n",
    "    #     break\n",
    "\n",
    "# Now `stacked_dataframes` is a list of stacked dataframes with additional columns for the components.\n",
    "\n",
    "# stack everything => same name for ram issue...\n",
    "stacked_dataframes = pd.concat(stacked_dataframes)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 40230000 entries, 0 to 223499\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   mu_hat         float64\n",
      " 1   sigma_hat      float64\n",
      " 2   MMD            float64\n",
      " 3   b              float64\n",
      " 4   time           float64\n",
      " 5   mu_hat_MLE     float64\n",
      " 6   sigma_hat_MLE  float64\n",
      " 7   mu             int32  \n",
      " 8   sigma          int32  \n",
      " 9   xsize          int32  \n",
      " 10  ysize          int32  \n",
      " 11  ID             int32  \n",
      "dtypes: float64(7), int32(5)\n",
      "memory usage: 3.1 GB\n"
     ]
    }
   ],
   "source": [
    "stacked_dataframes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\G'\n",
      "C:\\Users\\nickk\\AppData\\Local\\Temp\\ipykernel_9688\\1053996415.py:5: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  stacked_dataframes.to_csv(f\"{saving_path}\\Gauss_Processed_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "# 5 step: safe the processed DataFrame\n",
    "file_name = \"Gauss_Processed_data\"\n",
    "saving_path = r\"G:\\My Drive\\Studium\\UNIGE_Master\\Thesis\\Master_Thesis\\Data\\Gaussian_Data_processed\"\n",
    "\n",
    "stacked_dataframes.to_csv(f\"{saving_path}\\{file_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\G'\n",
      "C:\\Users\\nickk\\AppData\\Local\\Temp\\ipykernel_9688\\3402548225.py:1: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  stacked_dataframes.to_pickle(f\"{saving_path}\\Gauss_Processed_data.pkl\")\n"
     ]
    }
   ],
   "source": [
    "stacked_dataframes.to_pickle(f\"{saving_path}\\{file_name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
